---
title: "Takehome_Ex03"
editor: visual
---

# Take-home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods

## Background

Housing is an essential component of household wealth worldwide. Buying a housing has always been a major investment for most people. The price of housing is affected by many factors. Some of them are global in nature such as the general economy of a country or inflation rate. Others can be more specific to the properties themselves. These factors can be further divided to structural and locational factors. Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property. Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.

Conventional, housing resale prices predictive models were built by using [**Ordinary Least Square (OLS)**](https://en.wikipedia.org/wiki/Ordinary_least_squares) method. However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions. With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998). In view of this limitation, **Geographical Weighted Models** were introduced for calibrating predictive model for housing resale prices.

## Objective

To predict HDB resale prices at the sub-market level for the month of January and February 2023 in Singapore.The predictive models will be built by using by using conventional OLS method and GWR methods. The objective is to compare the performance of the conventional OLS method versus the geographical weighted methods.

## Concept for GWR, Hedonic pricing model

**Geographically weighted regression (GWR)** is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics)

In this take home assignment, we need to build predictive model using GWR. The predictive model need to take consideration into locational factors such as proximity to childcare centre, public transport service and shopping centre.

Hence, we have to first identify the relevant location factors for this assignment, which including:

-   Proximity to CBD

-   Proximity to eldercare

-   Proximity to foodcourt/hawker centres

-   Proximity to MRT

-   Proximity to park

-   Proximity to good primary school

-   Proximity to shopping mall

-   Proximity to supermarket

-   Numbers of kindergartens within 350m

-   Numbers of childcare centres within 350m

-   Numbers of bus stop within 350m

-   Numbers of primary school within 1km

## Steps

### Load Necessary Library

```{r}
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary,readxl,jsonlite,rvest)
```

### **Importing geospatial data (MP2019)**

```{r}
mpsz2019 = st_read(dsn = "data/geospatial", layer = "MPSZ-2019")

```

### Updating CRS information

```{r}
mpsz_svy21 <- st_transform(mpsz2019, 3414)
```

```{r}
st_crs(mpsz_svy21)
```

### Check invalid geometries

```{r}
length(which(st_is_valid(mpsz_svy21) == FALSE))
```

Make the invalid geometries valid

```{r}
mpsz_svy21 <- st_make_valid(mpsz_svy21)
length(which(st_is_valid(mpsz_svy21) == FALSE))
```

### Aspatial Data Wrangling

#### Importing the aspatial data

```{r}
resale = read_csv("data/aspatial/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv")
```

We plan to look at **four-room flat** and a transaction period between1st January 2021 to 31st December 2022.

```{r}
resale <- resale %>% 
  filter(flat_type == "4 ROOM") %>%
  filter(month >= "2021-01" & month < "2023-01")
resale
```

However, when we look at the code, there is no Longitude and Latitude Information, which means we need to geocode it, to make sure we get accurate geocode result, we have to combine the Block and Street Name as input:

```{r}
resale$street_name <- gsub("ST\\.", "SAINT", resale$street_name)
```

```{r}
# 
# library(httr)
# geocode <- function(block, streetname) {
#   base_url <- "https://developers.onemap.sg/commonapi/search"
#   address <- paste(block, streetname, sep = " ")
#   query <- list("searchVal" = address, 
#                 "returnGeom" = "Y",
#                 "getAddrDetails" = "N",
#                 "pageNum" = "1")
#   
#   res <- GET(base_url, query = query)
#   restext<-content(res, as="text")
#   
#   output <- fromJSON(restext)  %>% 
#     as.data.frame %>%
#     select(results.LATITUDE, results.LONGITUDE)
# 
#   return(output)
# }


```

```{r}
# resale$LATITUDE <- 0
# resale$LONGITUDE <- 0
# 
# for (i in 1:nrow(resale)){
#   temp_output <- geocode(resale[i, 4], resale[i, 5])
#   
#   resale$LATITUDE[i] <- temp_output$results.LATITUDE
#   resale$LONGITUDE[i] <- temp_output$results.LONGITUDE}

```

**Make a copy for the geocoded data:**

```{r}
#write_csv(resale, "data/rds/resalecopy1.csv")
```

#### Converting aspatial data frame into a sf object

```{r}
resale = read_csv("data/rds/resalecopy1.csv")
```

```{r}
view(resale)
```

```{r}
resale.sf <- st_as_sf(resale,
                            coords = c("LONGITUDE", "LATITUDE"),
                            crs=4326) %>%
  st_transform(crs=3414)
```

```{r}
rs_transform <- resale.sf %>%
  mutate(resale.sf, address = paste(block,street_name)) %>%
  mutate(resale.sf, remaining_lease_yr = as.integer(str_sub(remaining_lease, 0, 2))) %>%
  mutate(resale.sf, remaining_lease_mth = as.integer(str_sub(remaining_lease, 9, 11)))
```

```{r}
add_list <- sort(unique(rs_transform$address))
```

### Locational Factor

#### CBD

As the proximity to CBD is one of the locational factor we interested in to improve our predicted model, let's take the coordinates of Downtown core to be the coordinates of CBD

```{r}
lat <- 1.287953
lng <- 103.851784

cbd_sf <- data.frame(lat, lng) %>%
  st_as_sf(coords = c("lng", "lat"), crs=4326) %>%
  st_transform(crs=3414)
```

#### Eldercare

```{r}
Eldercare_sf <- st_read(dsn = "data/geospatial/eldercare-services-shp", 
                layer = "ELDERCARE")
```

```{r}
st_crs(Eldercare_sf)
```

```{r}
Eldercare_sf <- st_transform(Eldercare_sf, crs=3414)
```

#### **Kindergartens**

```{r}
library(sf)
library(onemapsgapi)

token <- "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOjEwMDUyLCJ1c2VyX2lkIjoxMDA1MiwiZW1haWwiOiJ0YWtvdGFrb3YwMDBAZ21haWwuY29tIiwiZm9yZXZlciI6ZmFsc2UsImlzcyI6Imh0dHA6XC9cL29tMi5kZmUub25lbWFwLnNnXC9hcGlcL3YyXC91c2VyXC9zZXNzaW9uIiwiaWF0IjoxNjc5NTU4NDYxLCJleHAiOjE2Nzk5OTA0NjEsIm5iZiI6MTY3OTU1ODQ2MSwianRpIjoiZDgyZTg1MDMzMTUzMTRkZjEzYjk5MWJmMDJkMDQ1NjIifQ.070RoJrraz95GuLVvYZpfzyMyGWQZ6S0D5FsLL39WGU"
```

```{r}
search_themes(token, "kindergartens") 
```

```{r}
Kindergartens_tibble <- get_theme(token, "kindergartens")
```

```{r}
Kindergartens_sf <- st_as_sf(Kindergartens_tibble, coords=c("Lng", "Lat"), crs=4326)
```

```{r}
Kindergartens_sf <- st_transform(Kindergartens_sf, crs=3414)
```

#### Childcare Center

```{r}
search_themes(token, "childcare")
```

```{r}

library(sf)
library(onemapsgapi)

themetibble <- get_theme(token, "childcare")
childcaresf <- st_as_sf(themetibble, coords=c("Lng", "Lat"), crs=4326)

```

```{r}
childcaresf <- st_transform(childcaresf, crs=3414)
```

### Park

```{r}
search_themes(token, "parks")
```

```{r}
library(sf)
library(onemapsgapi)
Parks_themetibble <- get_theme(token, "nationalparks")
Parks_sf <- st_as_sf(Parks_themetibble, coords=c("Lng", "Lat"), crs=4326)
```

```{r}
Parks_sf <- st_transform(Parks_sf, crs=3414)
```

#### Shopping mall

Get Call function

```{r}
get_coords <- function(add_list){
  
  # Create a data frame to store all retrieved coordinates
  postal_coords <- data.frame()
    
  for (i in add_list){
    #print(i)

    r <- GET('https://developers.onemap.sg/commonapi/search?',
           query=list(searchVal=i,
                     returnGeom='Y',
                     getAddrDetails='Y'))
    data <- fromJSON(rawToChar(r$content))
    found <- data$found
    res <- data$results
    
    # Create a new data frame for each address
    new_row <- data.frame()
    
    # If single result, append 
    if (found == 1){
      postal <- res$POSTAL 
      lat <- res$LATITUDE
      lng <- res$LONGITUDE
      new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
    }
    
    # If multiple results, drop NIL and append top 1
    else if (found > 1){
      # Remove those with NIL as postal
      res_sub <- res[res$POSTAL != "NIL", ]
      
      # Set as NA first if no Postal
      if (nrow(res_sub) == 0) {
          new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
      }
      
      else{
        top1 <- head(res_sub, n = 1)
        postal <- top1$POSTAL 
        lat <- top1$LATITUDE
        lng <- top1$LONGITUDE
        new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
      }
    }

    else {
      new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
    }
    
    # Add the row
    postal_coords <- rbind(postal_coords, new_row)
  }
  return(postal_coords)
}
```

```{r}
url <- "https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore"
malls_list <- list()

for (i in 2:7){
  malls <- read_html(url) %>%
    html_nodes(xpath = paste('//*[@id="mw-content-text"]/div[1]/div[',as.character(i),']/ul/li',sep="") ) %>%
    html_text()
  malls_list <- append(malls_list, malls)
}
```

```{r}
library(httr)
malls_list_coords <- get_coords(malls_list) %>% 
  rename("mall_name" = "address")
```

```{r}
malls_list_coords <- subset(malls_list_coords, mall_name!= "Yew Tee Shopping Centre")
```

```{r}
invalid_malls<- subset(malls_list_coords, is.na(malls_list_coords$postal))
invalid_malls_list <- unique(invalid_malls$mall_name)
corrected_malls <- c("Clarke Quay", "City Gate", "Raffles Holland V", "Knightsbridge", "Mustafa Centre", "GR.ID", "Shaw House",
                     "The Poiz Centre", "Velocity @ Novena Square", "Singapore Post Centre", "PLQ Mall", "KINEX", "The Grandstand")

for (i in 1:length(invalid_malls_list)) {
  malls_list_coords <- malls_list_coords %>% 
    mutate(mall_name = ifelse(as.character(mall_name) == invalid_malls_list[i], corrected_malls[i], as.character(mall_name)))
}
```

```{r}
malls_list <- sort(unique(malls_list_coords$mall_name))
```

```{r}
malls_coords <- get_coords(malls_list)
```

```{r}
malls_coords[(is.na(malls_coords$postal) | is.na(malls_coords$latitude) | is.na(malls_coords$longitude)), ]
```

```{r}
malls_sf <- st_as_sf(malls_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

#### Primary School

```{r}
pri_sch <- read_csv("data/geospatial/general-information-of-schools.csv")
```

```{r}
pri_sch <- pri_sch %>%
  filter(mainlevel_code == "PRIMARY"| mainlevel_code == "MIXED LEVELS") %>%
  select(school_name, address, postal_code, mainlevel_code)
```

```{r}
prisch_list <- sort(unique(pri_sch$postal_code))
```

```{r}
prisch_coords <- get_coords(prisch_list)
```

```{r}
prisch_coords[(is.na(prisch_coords$postal) | is.na(prisch_coords$latitude) | is.na(prisch_coords$longitude)), ]
```

```{r}
prisch_coords = prisch_coords[c("postal","latitude", "longitude")]
pri_sch <- left_join(pri_sch, prisch_coords, by = c('postal_code' = 'postal'))
```

```{r}
prisch_sf <- st_as_sf(pri_sch,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

#### Select top 10 school

```{r}
postal_codes <- c(599986, 449761, 597610, 536451, 579767, 128806, 569405, 738907, 579646, 227988)

# Filter prisch_sf for rows with postal code in the vector
top10prisch_sf <- prisch_sf %>%
  filter(postal_code %in% postal_codes)
top10prisch_sf
```

#### Hawker Center

```{r}
hawker_sf <- st_read("data/geospatial/hawker-centres-kml.kml") 
```

```{r}
hawker_sf <- st_transform(hawker_sf, crs=3414)
```

```{r}
st_crs(hawker_sf)
```

#### Bus Stop

```{r}
bus_sf <- st_read(dsn="data/geospatial/BusStop_Feb2023", layer="BusStop")
```

```{r}
bus_sf <- st_transform(bus_sf, crs=3414)
```

#### Mrt

```{r}
rail_network_sf <- st_read(dsn="data/geospatial/TrainStation_Feb2023", layer="RapidTransitSystemStation")
```

```{r}
rail_network_sf<- st_transform(rail_network_sf, crs=3414)
```

```{r}
supermkt_sf <- st_read("data/geospatial/supermarkets-geojson.geojson")
```

```{r}
supermkt_sf<- st_transform(supermkt_sf, crs=3414)
```

#### Check invalid geometry for locational factor

```{r}
length(which(st_is_valid(bus_sf) == FALSE))
```

```{r}
# # Identify the invalid geometry
# library(leaflet)
# invalid_geom <- which(!st_is_valid(rail_network_sf))
# 
# # Use st_cast() to convert the invalid geometry to a valid geometry type
# rail_network_sf <- st_cast(rail_network_sf, "MULTILINESTRING")
# 
# # Use shiny to manually edit the geometry
# library(shiny)
# shinyApp(
#   ui = fluidPage(
#     leafletOutput("map")
#   ),
#   server = function(input, output) {
#     output$map <- renderLeaflet({
#       leaflet(rail_network_sf) %>%
#         addTiles() %>%
#         addPolylines()
#     })
#     observe({
#       if (!is.null(input$map_shape_click)) {
#         clicked_shape <- input$map_shape_click
#         if (clicked_shape$type == "polyline") {
#           # Edit the clicked polyline shape
#           edited_shape <- leafletProxy("map") %>%
#             editPolyline(target = clicked_shape$id) %>%
#             asJSON()
#           rail_network_sf <- st_as_sf(fromJSON(edited_shape))
#         }
#       }
#     })
#   }
# )
# 
# # Make the geometry valid
# rail_network_sf <- st_make_valid(rail_network_sf)
```

```{r}
length(which(st_is_valid(rail_network_sf) == FALSE))
```

```{r}
length(which(st_is_valid(childcaresf) == FALSE))
```

```{r}
length(which(st_is_valid(Eldercare_sf) == FALSE))
```

```{r}
length(which(st_is_valid(cbd_sf)))
```

```{r}
cbd_sf <- st_make_valid(cbd_sf)
length(which(st_is_valid(cbd_sf) == FALSE))
```

```{r}
length(which(st_is_valid(childcaresf) == FALSE))

```

```{r}
length(which(st_is_valid(Kindergartens_sf) == FALSE))
```

```{r}
length(which(st_is_valid(malls_sf) == FALSE))
```

```{r}
length(which(st_is_valid(Parks_sf) == FALSE))
```

```{r}
length(which(st_is_valid(prisch_sf) == FALSE))
```

```{r}
length(which(st_is_valid(top10prisch_sf) == FALSE))
```

```{r}
tmap_mode("view")
tm_shape(hawker_sf) +
  tm_dots(alpha=0.5, #affects transparency of points
          col="#d62828",
          size=0.05) +
tm_shape(Parks_sf) +
  tm_dots(alpha=0.5,
          col="#f77f00",
          size=0.05) +
tm_shape(malls_sf) +
  tm_dots(alpha=0.5,
          col="#eae2b7",
          size=0.05)
```

```{r}
tmap_mode("view")
tm_shape(childcaresf) +
  tm_dots(alpha=0.5, #affects transparency of points
          col="#2ec4b6",
          size=0.05) +
tm_shape(Eldercare_sf) +
  tm_dots(alpha=0.5,
          col="#e71d36",
          size=0.05) +
tm_shape(Kindergartens_sf) +
  tm_dots(alpha=0.5,
          col="#ff9f1c",
          size=0.05) +
tm_shape(top10prisch_sf) +
  tm_dots(alpha=0.5,
        col="#f4acb7",
        size=0.05)
```

### Structural Factors

```{r}
library(units)
proximity <- function(df1, df2, varname) {
  dist_matrix <- st_distance(df1, df2) %>%
    drop_units()
  df1[,varname] <- rowMins(dist_matrix)
  return(df1)
}
```

```{r}
install.packages("matrixStats")
library(matrixStats)
```

```{r}
proximity <- function(df1, df2, varname) {
  dist_matrix <- st_distance(df1, df2) %>%
    drop_units()
  df1[,varname] <- rowMins(dist_matrix)
  return(df1)
}
```

```{r}
# resale.sf <- 
#   # the columns will be truncated later on when viewing 
#   # so we're limiting ourselves to two-character columns for ease of viewing between
#   proximity(resale.sf, cbd_sf, "PROX_CBD") %>%
#   proximity(., childcaresf, "PROX_CHILDCARE") %>%
#   proximity(., Eldercare_sf, "PROX_ELDERCARE") %>%
#   proximity(., hawker_sf, "PROX_HAWKER") %>%
#   proximity(., rail_network_sf, "PROX_MRT") %>%
#   proximity(., Parks_sf, "PROX_PARK") %>%
#   proximity(., top10prisch_sf, "PROX_TOPPRISCH") %>%
#   proximity(., malls_sf, "PROX_MALL") %>%
#   proximity(., supermkt_sf, "PROX_SPRMKT") 

```

```{r}
# num_radius <- function(df1, df2, varname, radius) {
#   dist_matrix <- st_distance(df1, df2) %>%
#     drop_units() %>%
#     as.data.frame()
#   df1[,varname] <- rowSums(dist_matrix <= radius)
#   return(df1)
# }
```

```{r}
# resale.sf <- 
#   num_radius(resale.sf, Kindergartens_sf, "NUM_KNDRGTN", 350) %>%
#   num_radius(., childcaresf, "NUM_CHILDCARE", 350) %>%
#   num_radius(., bus_sf, "NUM_BUS_STOP", 350) %>%
#   num_radius(., prisch_sf, "NUM_PriSch", 1000)
```

```{r}
# resale.sf <- resale.sf %>%
#   # sometimes, you might get an internal error: can't find `agr` columns
#   # to circumvent, have an empty mutate()
#   # you won't affect your columns, but it corrects the agr attribute names
#   # reference: https://github.com/r-spatial/sf/issues/1472
#   mutate() %>%
#   rename("AREA_SQM" = "floor_area_sqm", 
#          "LEASE_YRS" = "remaining_lease", 
#          "PRICE"= "resale_price") %>%
#   relocate(`PRICE`)
```

The figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower price, and we need to normalize it by using log transformation

```{r}
# st_write(resale.sf, "data/geospatial/resale-allfinal.shp")
```

```{r}
resale_sf <- st_read(dsn="data/geospatial", layer="resale-allfinal")
```

### Drawing Statistical Point Map

FLOOR LEVEL

### Hedonic Pricing Modelling

#### Simple Linear Regression Method

```{r}

```
