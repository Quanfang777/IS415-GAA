---
title: "Takehome_Ex03"
editor: visual
---

# Take-home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods

## Background

Housing is an essential component of household wealth worldwide. Buying a housing has always been a major investment for most people. The price of housing is affected by many factors. Some of them are global in nature such as the general economy of a country or inflation rate. Others can be more specific to the properties themselves. These factors can be further divided to structural and locational factors. Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property. Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.

Conventional, housing resale prices predictive models were built by using [**Ordinary Least Square (OLS)**](https://en.wikipedia.org/wiki/Ordinary_least_squares) method. However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions. With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998). In view of this limitation, **Geographical Weighted Models** were introduced for calibrating predictive model for housing resale prices.

## Objective

To predict HDB resale prices at the sub-market level for the month of January and February 2023 in Singapore.The predictive models will be built by using by using conventional OLS method and GWR methods. The objective is to compare the performance of the conventional OLS method versus the geographical weighted methods.

## Concept for GWR, Hedonic pricing model

**Geographically weighted regression (GWR)** is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics)

In this take home assignment, we need to build predictive model using GWR. The predictive model need to take consideration into locational factors such as proximity to childcare centre, public transport service and shopping centre.

Hence, we have to first identify the relevant location factors for this assignment, which including:

-   Proximity to CBD

-   Proximity to eldercare

-   Proximity to foodcourt/hawker centres

-   Proximity to MRT

-   Proximity to park

-   Proximity to good primary school

-   Proximity to shopping mall

-   Proximity to supermarket

-   Numbers of kindergartens within 350m

-   Numbers of childcare centres within 350m

-   Numbers of bus stop within 350m

-   Numbers of primary school within 1km

## Steps

### Load Necessary Library

```{r}
pacman::p_load(olsrr, corrplot, sf, spdep, GWmodel, tmap, tidyverse, gtsummary,readxl,jsonlite,rvest)
```

### **Importing geospatial data (MP2019)**

```{r}
mpsz2019 = st_read(dsn = "data/geospatial", layer = "MPSZ-2019")

```

### Updating CRS information

```{r}
mpsz_svy21 <- st_transform(mpsz2019, 3414)
```

check whether the CRS has correctly assigned:

```{r}
st_crs(mpsz_svy21)
```

### Check invalid geometries

```{r}
length(which(st_is_valid(mpsz_svy21) == FALSE))
```

### Make the invalid geometries valid

```{r}
mpsz_svy21 <- st_make_valid(mpsz_svy21)
length(which(st_is_valid(mpsz_svy21) == FALSE))
```

### Aspatial Data Wrangling

#### Importing the aspatial data

```{r}
resale = read_csv("data/aspatial/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv")
```

We plan to look at **four-room flat** and a transaction period between1st January 2021 to 31st December 2022.

```{r}
resale <- resale %>% 
  filter(flat_type == "4 ROOM") 
resale
```

However, when we look at the code, there is no Longitude and Latitude Information, which means we need to geocode it, to make sure we get accurate geocode result, we have to combine the **Block and Street Name** as input:

In OneMapSG API, "ST." is usually written as "SAINT" instead, so it is better for us to replace ST. with SAINT

```{r}
resale$street_name <- gsub("ST\\.", "SAINT", resale$street_name)
```

Here is the code for geocoding the aspatial data: (Reference was taken from the senior sample submissions for the code for this section, with credit to MEGAN SIM TZE YEN)

```{r}
# 
# library(httr)
# geocode <- function(block, streetname) {
#   base_url <- "https://developers.onemap.sg/commonapi/search"
#   address <- paste(block, streetname, sep = " ")
#   query <- list("searchVal" = address, 
#                 "returnGeom" = "Y",
#                 "getAddrDetails" = "N",
#                 "pageNum" = "1")
#   
#   res <- GET(base_url, query = query)
#   restext<-content(res, as="text")
#   
#   output <- fromJSON(restext)  %>% 
#     as.data.frame %>%
#     select(results.LATITUDE, results.LONGITUDE)
# 
#   return(output)
# }


```

```{r}
# resale$LATITUDE <- 0
# resale$LONGITUDE <- 0
# 
# for (i in 1:nrow(resale)){
#   temp_output <- geocode(resale[i, 4], resale[i, 5])
#   
#   resale$LATITUDE[i] <- temp_output$results.LATITUDE
#   resale$LONGITUDE[i] <- temp_output$results.LONGITUDE}

```

**Make a copy for the geocoded data:**

Make a copy for the geocoded data so we don't need to run the previous process again:

```{r}
#write_csv(resale, "data/rds/resalecopy1.csv")
```

#### Read the geocoded data

```{r}
resale = read_csv("data/rds/resalecopy1.csv")
```

### Update CRS

```{r}
resale.sf <- st_as_sf(resale,
                            coords = c("LONGITUDE", "LATITUDE"),
                            crs=4326) %>%
  st_transform(crs=3414)
```

### Structural Factors

#### FLOOR LEVEL

```{r}
unique(resale$storey_range)
```

We can observe that there are 17 categories of storey ranges. To represent each range, we will use the pivot_wider() function to create duplicate variables, assigning a value of 1 if the observation falls within the range and 0 if it does not.

```{r}
resale <- resale %>%
  pivot_wider(names_from = storey_range, values_from = storey_range, 
              values_fn = list(storey_range = ~1), values_fill = 0) 
```

#### REMAINING LEASE

The current format of the remaining_lease variable is a string, but we need it to be in numeric format. To achieve this, we can split the string into its month and year values, calculate the remaining lease in years, and replace the original string values with the calculated numeric values.

```{r}
str_list <- str_split(resale$remaining_lease, " ")

for (i in 1:length(str_list)) {
  if (length(unlist(str_list[i])) > 2) {
      year <- as.numeric(unlist(str_list[i])[1])
      month <- as.numeric(unlist(str_list[i])[3])
      resale$remaining_lease[i] <- year + round(month/12, 2)
  }
  else {
    year <- as.numeric(unlist(str_list[i])[1])
    resale$remaining_lease[i] <- year
  }
}
```

```{r}
resale_sf <- st_as_sf(resale, 
                      coords = c("LONGITUDE", 
                                 "LATITUDE"), 
                      crs=4326) %>%
  st_transform(crs = 3414)
```

### Locational Factor

#### CBD

As the proximity to CBD is one of the locational factor we interested in to improve our predicted model, let's take the coordinates of Downtown core to be the coordinates of CBD

```{r}
lat <- 1.287953
lng <- 103.851784

cbd_sf <- data.frame(lat, lng) %>%
  st_as_sf(coords = c("lng", "lat"), crs=4326) %>%
  st_transform(crs=3414)
```

#### Eldercare

```{r}
Eldercare_sf <- st_read(dsn = "data/geospatial/eldercare-services-shp", 
                layer = "ELDERCARE")
```

```{r}
st_crs(Eldercare_sf)
```

```{r}
Eldercare_sf <- st_transform(Eldercare_sf, crs=3414)
```

#### **Kindergartens**

To get the Kindergartens' geometry data, I refer to Megan's method of using ONEMAP API to do the geocoding

The step including:

1.  Get the token from ONEMAP API
2.  Search the themes and check the themes available
3.  Using get_theme function to get the geospatial information of the Kindergarten

```{r}
# library(sf)
# library(onemapsgapi)
# 
# token <- "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOjEwMDUyLCJ1c2VyX2lkIjoxMDA1MiwiZW1haWwiOiJ0YWtvdGFrb3YwMDBAZ21haWwuY29tIiwiZm9yZXZlciI6ZmFsc2UsImlzcyI6Imh0dHA6XC9cL29tMi5kZmUub25lbWFwLnNnXC9hcGlcL3YyXC91c2VyXC9zZXNzaW9uIiwiaWF0IjoxNjc5NTU4NDYxLCJleHAiOjE2Nzk5OTA0NjEsIm5iZiI6MTY3OTU1ODQ2MSwianRpIjoiZDgyZTg1MDMzMTUzMTRkZjEzYjk5MWJmMDJkMDQ1NjIifQ.070RoJrraz95GuLVvYZpfzyMyGWQZ6S0D5FsLL39WGU"
```

```{r}
# search_themes(token, "kindergartens") 
```

```{r}
# Kindergartens_tibble <- get_theme(token, "kindergartens")
```

```{r}
# Kindergartens_sf <- st_as_sf(Kindergartens_tibble, coords=c("Lng", "Lat"), crs=4326)
```

```{r}
# Kindergartens_sf <- st_transform(Kindergartens_sf, crs=3414)
```

#### Childcare Center

```{r}
# search_themes(token, "childcare")
```

```{r}

# library(sf)
# library(onemapsgapi)
# 
# themetibble <- get_theme(token, "childcare")
# childcaresf <- st_as_sf(themetibble, coords=c("Lng", "Lat"), crs=4326)

```

```{r}
# childcaresf <- st_transform(childcaresf, crs=3414)
```

### Park

```{r}
# search_themes(token, "parks")
```

```{r}
# library(sf)
# library(onemapsgapi)
# Parks_themetibble <- get_theme(token, "nationalparks")
# Parks_sf <- st_as_sf(Parks_themetibble, coords=c("Lng", "Lat"), crs=4326)
```

```{r}
# Parks_sf <- st_transform(Parks_sf, crs=3414)
```

#### Shopping mall

(Reference was taken from the senior sample submissions for the code for this section, with credit to NOR AISYAH BINTE AJIT) The approach is about extracting the names of the malls from Wikipedia and then use get_coords function to obtain the respective coordinates

The following code chunk performs three steps:

-   Reads the Wikipedia HTML page containing the Shopping Malls in Singapore.

-   Reads the text portion of the unordered list element selected by html_nodes().

-   Appends the extracted mall names to an empty mall_list.

```{r}
# get_coords <- function(add_list){
#   
#   # Create a data frame to store all retrieved coordinates
#   postal_coords <- data.frame()
#     
#   for (i in add_list){
#     #print(i)
# 
#     r <- GET('https://developers.onemap.sg/commonapi/search?',
#            query=list(searchVal=i,
#                      returnGeom='Y',
#                      getAddrDetails='Y'))
#     data <- fromJSON(rawToChar(r$content))
#     found <- data$found
#     res <- data$results
#     
#     # Create a new data frame for each address
#     new_row <- data.frame()
#     
#     # If single result, append 
#     if (found == 1){
#       postal <- res$POSTAL 
#       lat <- res$LATITUDE
#       lng <- res$LONGITUDE
#       new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
#     }
#     
#     # If multiple results, drop NIL and append top 1
#     else if (found > 1){
#       # Remove those with NIL as postal
#       res_sub <- res[res$POSTAL != "NIL", ]
#       
#       # Set as NA first if no Postal
#       if (nrow(res_sub) == 0) {
#           new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
#       }
#       
#       else{
#         top1 <- head(res_sub, n = 1)
#         postal <- top1$POSTAL 
#         lat <- top1$LATITUDE
#         lng <- top1$LONGITUDE
#         new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
#       }
#     }
# 
#     else {
#       new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
#     }
#     
#     # Add the row
#     postal_coords <- rbind(postal_coords, new_row)
#   }
#   return(postal_coords)
# }
```

```{r}
# url <- "https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore"
# malls_list <- list()
# 
# for (i in 2:7){
#   malls <- read_html(url) %>%
#     html_nodes(xpath = paste('//*[@id="mw-content-text"]/div[1]/div[',as.character(i),']/ul/li',sep="") ) %>%
#     html_text()
#   malls_list <- append(malls_list, malls)
# }
```

```{r}
# library(httr)
# malls_list_coords <- get_coords(malls_list) %>% 
#   rename("mall_name" = "address")
```

Some of the names of the shopping malls in our dataset are not up-to-date, which can cause issues in the analysis. For example, POMO has been renamed to GR.ID and OD Mall has been renamed to The Grandstand. Hence, we should address these issues before proceeding. One of the shopping malls, Yew Tee Shopping Centre, was not found when researched further, so we should remove this row from our dataset

```{r}
# malls_list_coords <- subset(malls_list_coords, mall_name!= "Yew Tee Shopping Centre")
```

```{r}
# invalid_malls<- subset(malls_list_coords, is.na(malls_list_coords$postal))
# invalid_malls_list <- unique(invalid_malls$mall_name)
# corrected_malls <- c("Clarke Quay", "City Gate", "Raffles Holland V", "Knightsbridge", "Mustafa Centre", "GR.ID", "Shaw House",
#                      "The Poiz Centre", "Velocity @ Novena Square", "Singapore Post Centre", "PLQ Mall", "KINEX", "The Grandstand")
# 
# for (i in 1:length(invalid_malls_list)) {
#   malls_list_coords <- malls_list_coords %>% 
#     mutate(mall_name = ifelse(as.character(mall_name) == invalid_malls_list[i], corrected_malls[i], as.character(mall_name)))
# }
```

Then create a list to store unique names

```{r}
# malls_list <- sort(unique(malls_list_coords$mall_name))
```

To retrieve the coordinates of shopping malls

```{r}
# malls_coords <- get_coords(malls_list)
```

```{r}
# malls_coords[(is.na(malls_coords$postal) | is.na(malls_coords$latitude) | is.na(malls_coords$longitude)), ]
```

```{r}
# malls_sf <- st_as_sf(malls_coords,
#                     coords = c("longitude", 
#                                "latitude"),
#                     crs=4326) %>%
#   st_transform(crs = 3414)
```

#### Primary School

```{r}
# pri_sch <- read_csv("data/geospatial/general-information-of-schools.csv")
```

```{r}
# pri_sch <- pri_sch %>%
#   filter(mainlevel_code == "PRIMARY"| mainlevel_code == "MIXED LEVELS") %>%
#   select(school_name, address, postal_code, mainlevel_code)
```

```{r}
# prisch_list <- sort(unique(pri_sch$postal_code))
```

```{r}
# prisch_coords <- get_coords(prisch_list)
```

```{r}
# prisch_coords[(is.na(prisch_coords$postal) | is.na(prisch_coords$latitude) | is.na(prisch_coords$longitude)), ]
```

```{r}
# prisch_coords = prisch_coords[c("postal","latitude", "longitude")]
# pri_sch <- left_join(pri_sch, prisch_coords, by = c('postal_code' = 'postal'))
```

```{r}
# prisch_sf <- st_as_sf(pri_sch,
#                     coords = c("longitude", 
#                                "latitude"),
#                     crs=4326) %>%
#   st_transform(crs = 3414)
```

#### Select top 10 school

I refer to this [website](https://www.salary.sg/2022/best-primary-schools-2022-by-popularity/) and check which are the exact top 10 primary schools and get their postal code, then I filtered them from the primary school dataset

```{r}
# postal_codes <- c(599986, 449761, 597610, 536451, 579767, 128806, 569405, 738907, 579646, 227988)
# 
# # Filter prisch_sf for rows with postal code in the vector
# top10prisch_sf <- prisch_sf %>%
#   filter(postal_code %in% postal_codes)
# top10prisch_sf
```

#### Hawker Center

```{r}
# hawker_sf <- st_read("data/geospatial/hawker-centres-kml.kml") 
```

```{r}
# hawker_sf <- st_transform(hawker_sf, crs=3414)
```

```{r}
# st_crs(hawker_sf)
```

#### Bus Stop

```{r}
# bus_sf <- st_read(dsn="data/geospatial/BusStop_Feb2023", layer="BusStop")
```

```{r}
# bus_sf <- st_transform(bus_sf, crs=3414)
```

#### Mrt

```{r}
# rail_network_sf <- st_read(dsn="data/geospatial/TrainStation_Feb2023", layer="RapidTransitSystemStation")
```

```{r}
# rail_network_sf<- st_transform(rail_network_sf, crs=3414)
```

#### SuperMarket

```{r}
# supermkt_sf <- st_read("data/geospatial/supermarkets-geojson.geojson")
```

```{r}
# supermkt_sf<- st_transform(supermkt_sf, crs=3414)
```

#### Check invalid geometry for locational factor

Bus stop

```{r}
# length(which(st_is_valid(bus_sf) == FALSE))
```

MRT

```{r}
# length(which(st_is_valid(rail_network_sf) == FALSE))
```

It got invalid geometry and here is the code to make it valid:

```{r}
# # Identify the invalid geometry
# library(leaflet)
# invalid_geom <- which(!st_is_valid(rail_network_sf))
# 
# # Use st_cast() to convert the invalid geometry to a valid geometry type
# rail_network_sf <- st_cast(rail_network_sf, "MULTILINESTRING")
# 
# # Use shiny to manually edit the geometry
# library(shiny)
# shinyApp(
#   ui = fluidPage(
#     leafletOutput("map")
#   ),
#   server = function(input, output) {
#     output$map <- renderLeaflet({
#       leaflet(rail_network_sf) %>%
#         addTiles() %>%
#         addPolylines()
#     })
#     observe({
```

```{r}
# length(which(st_is_valid(rail_network_sf) == FALSE))
```

```{r}
# length(which(st_is_valid(childcaresf) == FALSE))
```

ElderlyCare

```{r}
# length(which(st_is_valid(Eldercare_sf) == FALSE))
```

```{r}
# length(which(st_is_valid(cbd_sf)))
```

CBD

```{r}
# cbd_sf <- st_make_valid(cbd_sf)
# length(which(st_is_valid(cbd_sf) == FALSE))
```

Childcare

```{r}
# length(which(st_is_valid(childcaresf) == FALSE))

```

Kindergartens

```{r}
# length(which(st_is_valid(Kindergartens_sf) == FALSE))
```

Malls

```{r}
# length(which(st_is_valid(malls_sf) == FALSE))
```

Parks

```{r}
# length(which(st_is_valid(Parks_sf) == FALSE))
```

Primary school

```{r}
# length(which(st_is_valid(prisch_sf) == FALSE))
```

Top primary school

```{r}
# length(which(st_is_valid(top10prisch_sf) == FALSE))
```

### Calculate Proximity

```{r}
# library(units)
# proximity <- function(df1, df2, varname) {
#   dist_matrix <- st_distance(df1, df2) %>%
#     drop_units()
#   df1[,varname] <- rowMins(dist_matrix)
#   return(df1)
# }
```

```{r}
# install.packages("matrixStats")
# library(matrixStats)
```

```{r}
# proximity <- function(df1, df2, varname) {
#   dist_matrix <- st_distance(df1, df2) %>%
#     drop_units()
#   df1[,varname] <- rowMins(dist_matrix)
#   return(df1)
# }
```

```{r}
# resale.sf <-
#   proximity(resale.sf, cbd_sf, "PROX_CBD") %>%
#   proximity(., childcaresf, "PROX_CHILDCARE") %>%
#   proximity(., Eldercare_sf, "PROX_ELDERCARE") %>%
#   proximity(., hawker_sf, "PROX_HAWKER") %>%
#   proximity(., rail_network_sf, "PROX_MRT") %>%
#   proximity(., Parks_sf, "PROX_PARK") %>%
#   proximity(., top10prisch_sf, "PROX_TOPPRISCH") %>%
#   proximity(., malls_sf, "PROX_MALL") %>%
#   proximity(., supermkt_sf, "PROX_SPRMKT")

```

```{r}
# num_radius <- function(df1, df2, varname, radius) {
#   dist_matrix <- st_distance(df1, df2) %>%
#     drop_units() %>%
#     as.data.frame()
#   df1[,varname] <- rowSums(dist_matrix <= radius)
#   return(df1)
# }
```

```{r}
# resale.sf <- 
#   num_radius(resale.sf, Kindergartens_sf, "NUM_KNDRGTN", 350) %>%
#   num_radius(., childcaresf, "NUM_CHILDCARE", 350) %>%
#   num_radius(., bus_sf, "NUM_BUS_STOP", 350) %>%
#   num_radius(., prisch_sf, "NUM_PriSch", 1000)
```

```{r}
# resale.sf <- resale.sf %>%
#   mutate() %>%
#   rename("AREA_SQM" = "floor_area_sqm",
#          "LEASE_YRS" = "remaining_lease",
#          "PRICE"= "resale_price") %>%
#   relocate(`PRICE`)
```

```{r}
# st_write(resale.sf, "data/geospatial/resale-final_cleaned.shp")
```

```{r}
resale.sf <- st_read(dsn="data/geospatial", layer="resale-final_cleaned")
```

```{r}
resale.sf <- resale.sf %>%
  rename( "AREA_SQM"= "AREA_SQ",
          "LEASE_YRS" = "LEASE_Y" , 
          "PROX_CBD" = "PROX_CB",
          "PROX_CHILDCARE" = "PROX_CH" ,
          "PROX_ELDERCARE" = "PROX_EL",
          "PROX_HAWKER"= "PROX_HA" ,
        "PROX_PARK"=  "PROX_PA",
          "PROX_MRT" ="PROX_MR",
          "PROX_SPRMKT"= "PROX_SP",
        "PROX_MALL"= "PROX_MA",
         "NUM_KNDRGTN"= "NUM_KND" ,
         "NUM_CHILDCARE"="NUM_CHI" ,
        "NUM_BUS_STOP"="NUM_BUS" ,
         "PROX_TOPPRISCH"= "PROX_TO"
      
      ) %>%
  relocate(`PRICE`)
```

### Drawing Statistical Point Map

```{r}
tmap_mode("view")
tmap_options(check.and.fix = TRUE)
tm_shape(resale.sf) +  
  tm_dots(col = "PRICE",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

From the statistical point map, we could observe that the southern and central area of the Singapore has a higher HDB resale price around 600k SGD

### EDA

```{r}
ggplot(data=resale.sf, aes(x=`PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

The figure above reveals a right skewed distribution. This means that more 4 room units were transacted at relative lower prices.Statistically, the skewed dsitribution can be normalised by using log transformation. We could see that most buildings seem to fall within 400k- 600k SGDdollars

```{r}
resale.sf <- resale.sf %>%
  mutate(`LOG_PRICE` = log(PRICE))
```

```{r}
ggplot(data=resale.sf, aes(x=`LOG_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

```{r}
resale.sf <- resale.sf  %>%
  mutate(resale.sf, LEASE_YRS = as.integer(str_sub(LEASE_YRS, 0, 2)))
```

### Multiple Histogram Plots distribution of variables

Let's also look at the Multiple Histogram Plots distribution of variables

```{r}
library(ggpubr)
```

```{r}
AREA_SQM <- ggplot(data = resale.sf, aes(x = `AREA_SQM`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_CBD <- ggplot(data = resale.sf, aes(x = `PROX_CBD`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_CHILDCARE <- ggplot(data = resale.sf, aes(x = `PROX_CHILDCARE`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_ELDERCARE <- ggplot(data = resale.sf, aes(x = `PROX_ELDERCARE`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_HAWKER <- ggplot(data = resale.sf, aes(x = `PROX_HAWKER`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_MRT <- ggplot(data = resale.sf, aes(x = `PROX_MRT`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_PARK <- ggplot(data = resale.sf, aes(x = `PROX_PARK`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_TOPPRISCH <- ggplot(data = resale.sf, aes(x = `PROX_TOPPRISCH`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_MALL <- ggplot(data = resale.sf, aes(x = `PROX_MALL`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_SPRMKT <- ggplot(data = resale.sf, aes(x = `PROX_SPRMKT`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')


ggarrange(AREA_SQM, PROX_CBD, PROX_CHILDCARE, PROX_ELDERCARE, PROX_HAWKER, PROX_MRT, PROX_PARK, PROX_TOPPRISCH, PROX_MALL, PROX_SPRMKT, ncol = 2, nrow = 5)
```

```{r}
NUM_CHILDCARE <- ggplot(data = resale.sf, aes(x = `NUM_CHILDCARE`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

NUM_KNDRGTN <- ggplot(data = resale.sf, aes(x = `NUM_KNDRGTN`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

NUM_BUS_STOP <- ggplot(data = resale.sf, aes(x = `NUM_BUS_STOP`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

LEASE_YRS <- ggplot(data = resale.sf, aes(x = `LEASE_YRS`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

ggarrange(NUM_KNDRGTN, NUM_CHILDCARE, NUM_BUS_STOP,LEASE_YRS, ncol = 2, nrow = 2)
```

```{r}
summary(resale.sf)
```

### Hedonic Pricing Modelling

#### Multiple Linear Regression Method

```{r}
resale_nogeo <- resale.sf %>%
  st_drop_geometry() %>%
  select_if(is.numeric) 
corrplot::corrplot(cor(resale_nogeo[, 2:17]), 
                   diag = FALSE, 
                   order = "AOE",
                   tl.pos = "td", 
                   tl.cex = 0.5, 
                   method = "number", 
                   type = "upper")
```

Before building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as **multicollinearity in statistics**.

From the scatterplot matrix , we could see that we have "lease_commence_date" highly corrlated to "Lease_Years" so we decided to remove "lease_commence_date" to prevent the issue of **multicollinearity**

```{r}
#resale.sf<- select(resale.sf, -lease_commence_date)
```

### Data Sampling

```{r}
training_data<-resale.sf %>% 
  filter(month >= "2021-01" & month <= "2022-10")

testing_data<-resale.sf %>%
  filter(month >= "2022-10" & month <= "2022-12")
```

### Building a non-spatial multiple linear regression

```{r}
price_mlr <- lm(PRICE ~ AREA_SQM + PROX_CBD +  PROX_ELDERCARE + PROX_HAWKER+ PROX_MRT +  PROX_PARK + PROX_TOPPRISCH + PROX_MALL +PROX_SPRMKT +LEASE_YRS + NUM_CHILDCARE+NUM_KNDRGTN+ NUM_PrS,
                data=training_data)
write_rds(price_mlr, "data/ML_results/price_mlr.rds")
gtsummary::tbl_regression(price_mlr)
```

#### Building Hedonic Pricing Models using GWmodel

Converting the sf data.frame to SpatialPointDataFrame

```{r}
train_data_sp <- as_Spatial(training_data)
train_data_sp
```

```{r}
# bw_adaptive <- bw.gwr(PRICE ~ AREA_SQM + PROX_CBD +  PROX_ELDERCARE + PROX_HAWKER+ PROX_MRT +  PROX_PARK + PROX_TOPPRISCH + PROX_MALL +PROX_SPRMKT +LEASE_YRS + NUM_CHILDCARE + NUM_KNDRGTN,
#                   data=train_data_sp,
#                   approach="CV",
#                   kernel="gaussian",
#                   adaptive=TRUE,
#                   longlat=FALSE)
# 

```

```{r}
# saving the result as an rds object
# write_rds(bw_adaptive, "data/ML_results/bw_adaptive.rds")
```

Constructing the adaptive bandwidth gwr model

```{r}
bw_adaptive <- read_rds("data/ML_results/bw_adaptive.rds")
```

```{r}
# gwr_adaptive <- 
#   
#   gwr.basic(formula = PRICE ~ AREA_SQM + PROX_CBD +  PROX_ELDERCARE + PROX_HAWKER+ PROX_MRT +  PROX_PARK + PROX_TOPPRISCH + PROX_MALL +PROX_SPRMKT +LEASE_YRS + NUM_CHILDCARE + NUM_KNDRGTN,
#                           data=train_data_sp,
#                           bw=bw_adaptive, 
#                           kernel = 'gaussian', 
#                           adaptive=TRUE,
#                           longlat = FALSE)
```

```{r}
# write_rds(gwr_adaptive, "data/ML_results/gwr_adaptive.rds")
```

```{r}
gwr_adaptive <- read_rds("data/ML_results/gwr_adaptive.rds")
gwr_adaptive
```

### Preparing coordinates data

```{r}
coords <- st_coordinates(resale.sf)
coords_train <- st_coordinates(training_data)
coords_test <- st_coordinates(testing_data)
```

```{r}
coords_train <- write_rds(coords_train, "data/ML_results/coords_train.rds" )
coords_test <- write_rds(coords_test, "data/ML_results/coords_test.rds" )
```

```{r}
coords_train<-read_rds("data/ML_results/coords_train.rds")
coords_test<-read_rds("data/ML_results/coords_test.rds")
```

Dropping geometry field

```{r}
library(dplyr)
library(sf)
```

```{r}
train_data <- training_data %>% 
  st_drop_geometry()
write_rds(train_data, "data/ML_results/train_data.rds")
```

## Calibrating Random Forest Model

```{r}
pacman::p_load(sf, tmap, sfdep, tidyverse, olsrr, ggpubr, GWmodel, SpatialML, tidymodels, jsonlite, readxl, Rfast, corrplot, gtsummary, spdep, Metrics, ggplot)
```

```{r}
install.packages("grf")
library(grf)
```

```{r}
# set.seed(1234)
# bw_adaptive <- bw.gwr(PRICE ~ AREA_SQM + PROX_CBD +  PROX_ELDERCARE + PROX_HAWKER+ PROX_MRT +  PROX_PARK + PROX_TOPPRISCH + PROX_MALL +PROX_SPRMKT +LEASE_YRS + NUM_CHILDCARE + NUM_KNDRGTN,
#                       data=train_data_nogeom, 
#                       approach="CV", 
#                       kernel="gaussian", 
#                       adaptive=TRUE, 
#                       longlat=FALSE)
```

![](data/ML_results/GRFW.png)

```{r}
# set.seed(1234)
# grf_adaptive <- grf(formula = PRICE ~ AREA_SQM + PROX_CBD +  PROX_ELDERCARE + PROX_HAWKER+ PROX_MRT +  PROX_PARK + PROX_TOPPRISCH + PROX_MALL +PROX_SPRMKT +LEASE_YRS + NUM_CHILDCARE + NUM_KNDRGTN,
#                      dframe=train_data,
#                      bw=75,
#                      kernel="adaptive",
#                      coords=coords_train,
#                       ntree = 5)
```

```{r}
write_rds(grf_adaptive, "data/ML_results/grf_adaptive.rds")
```

```{r}
file.exists("data/ML_results/grf_adaptive.rds")
```

```{r}
grf_adaptive <- read_rds("data/ML_results/grf_adaptive.rds")

```

```{r}
class(grf_adaptive)
```

### Prediction

```{r}
test_data <- cbind(testing_data, coords_test) %>%
  st_drop_geometry()
```

```{r}
gwRF_pred <- predict.grf(grf_adaptive, 
                           test_data, 
                           x.var.name="X",
                           y.var.name="Y", 
                           local.w=1,
                           global.w=0)
```

```{r}
grf_pred <- write_rds(gwRF_pred, "data/ML_results/GRF_pred.rds")
```

```{r}
grf_pred <- read_rds("data/ML_results/GRF_pred.rds")
```

### Visualization

```{r}
test_data_plot <- cbind(test_data, grf_pred)
```

```{r}
ggplot(data = test_data_plot,
       aes(x = grf_pred,
           y = PRICE)) +
  geom_point() +
  geom_abline(col = "Red")
```

### Conclusion

Upon comparing two models, it was observed that the Random Forest Model outperformed the Linear Regression model. The results from the Random Forest Model suggest that, at a 5% significance level, except PROX_TOPPRISCH and PROX_SPRMKT, the other variables are significantly associated with the dependent variable PRICE. However, it is possible that there are other factors that were not accounted for in the analysis.
